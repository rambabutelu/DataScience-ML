{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**spaCy**<br>\n",
    "spaCy is an open-source Python library that parses and understands large volume of text. Models are available that cater to specific languages.\n",
    "\n",
    "Here we'll setup spaCy to work with python and explore some of it's features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Installation and Setup**<br>\n",
    "It's a two-step process. Installing spaCy using conda or pip being the first step and downloading the specific model (based on language) being the second.\n",
    "\n",
    "Note: I generally use pip(or pip3 for python 3.x versions).\n",
    "\n",
    "1. Installation from command line or terminal<br>\n",
    "   `pip3 install spacy`<br><br>\n",
    "2. Downloading the model from command line or terminal<br>\n",
    "    `python -m spacy download \"model_name\"`  (where model_name can be `en_core_web_sm, en_core_web_md, en` etc..)<br><br>\n",
    "    For models and more details : https://spacy.io/usage/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with spaCy and Python\n",
    "\n",
    "Below are the typical set of instructions for importing and working with spaCy. Model loading can take a bit of time as spaCy has fairly large librart to load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salesforce      PROPN    nsubj   \n",
      "announced       VERB     ROOT    \n",
      "its             DET      poss    \n",
      "acquisition     NOUN     dobj    \n",
      "of              ADP      prep    \n",
      "Tableau         PROPN    pobj    \n",
      "at              ADP      prep    \n",
      "an              DET      det     \n",
      "enterprise      NOUN     compound\n",
      "value           NOUN     pobj    \n",
      "of              ADP      prep    \n",
      "approximately   ADV      advmod  \n",
      "$               SYM      quantmod\n",
      "16              NUM      compound\n",
      "billion         NUM      pobj    \n",
      ".               PUNCT    punct   \n"
     ]
    }
   ],
   "source": [
    "# Importing spaCy and loading the language library\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Creating a document object\n",
    "document = nlp(u'Salesforce announced its acquisition of Tableau at an enterprise value of approximately $16 billion.')\n",
    "\n",
    "# Printing tokens/parts of the document created\n",
    "for token in document:\n",
    "    print(f\"{token.text:{15}} {token.pos_:{8}} {token.dep_:{8}}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above output doesn't seem very user-friendly, but we can see some interesting things.\n",
    "\n",
    "a. Salesforce is recognized to be a Proper Noun<br>\n",
    "b. 16 is recognized a Number\n",
    "\n",
    "As we go further we'll see how combined tokens such as `$16 billion` can be recognized as **money**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "When we run `nlp`, our text goes through a *pipeline* that bearks down the text and then performs a series of operations to tag, parse and describe the data.\n",
    "\n",
    "Pipelinne Information: https://spacy.io/usage/spacy-101#pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tagger', <spacy.pipeline.pipes.Tagger at 0x11368c9e8>),\n",
       " ('parser', <spacy.pipeline.pipes.DependencyParser at 0x11893eb88>),\n",
       " ('ner', <spacy.pipeline.pipes.EntityRecognizer at 0x11893eac8>)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the nlp pipeline\n",
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tagger', 'parser', 'ner']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see that the document is going through a tagger, a parser and a ner (Named Entity Recognition)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "The first step in processing text is to split up all the component parts (words & punctuation) into \"tokens\". These tokens are annotated inside the Document object to contain descriptive information.<br>\n",
    "Let's understand this with the help of an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harry      PROPN  nsubj \n",
      "is         VERB   aux   \n",
      "n't        ADV    neg   \n",
      "looking    VERB   ROOT  \n",
      "           SPACE        \n",
      "for        ADP    prep  \n",
      "jobs       NOUN   pobj  \n",
      "anymore    ADV    advmod\n",
      ".          PUNCT  punct \n"
     ]
    }
   ],
   "source": [
    "document2 = nlp(u\"Harry isn't looking  for jobs anymore.\")\n",
    "\n",
    "for token in document2:\n",
    "    print(f\"{token.text:{10}} {token.pos_:{6}} {token.dep_:{6}}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that `isn't` has been split into two tokens. spaCy is able to recognize the root verb `is` and the negation in it. Also the extended whitespace and the period at the end of the sentence are assigned respective tokens.<br><br>\n",
    "\n",
    "It can be noted that although document2 holds processed information, it also retains the original text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Harry isn't looking  for jobs anymore."
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Harry"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(document2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Part-of-Speech Tagging (POS)\n",
    "The next step after splitting the text up into tokens is to assign parts of speech. In the above example, `Harry` was recognized to be a *proper noun*. Here some statistical modeling is required. For example, words that follow \"the\" are typically nouns.\n",
    "\n",
    "For a full list of POS Tags: \n",
    " https://spacy.io/api/annotation#pos-tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PROPN'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document2[0].pos_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Dependencies\n",
    "There is also syntactic dependencies assigned to each token. `harry` is identified as an `nsubj` or the *nominal subject* of the sentence.\n",
    "\n",
    "For a full list of Syntactic Dependencies: https://spacy.io/api/annotation#dependency-parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nsubj'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document2[0].dep_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To see the description of the tags we can use `spacy.explain(tag)`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'proper noun'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('PROPN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nominal subject'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('nsubj')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional token attributes\n",
    "\n",
    "|Tag|Description|doc2[0].tag|\n",
    "|:------|:------:|:------|\n",
    "|`.text`|The original word text<!-- .element: style=\"text-align:left;\" -->|`Harry`|\n",
    "|`.lemma_`|The base form of the word|`harry`|\n",
    "|`.pos_`|The simple part-of-speech tag|`PROPN`/`proper noun`|\n",
    "|`.tag_`|The detailed part-of-speech tag|`NNP`/`noun, proper singular`|\n",
    "|`.shape_`|The word shape â€“ capitalization, punctuation, digits|`Xxxxx`|\n",
    "|`.is_alpha`|Is the token an alpha character?|`True`|\n",
    "|`.is_stop`|Is the token part of a stop list, i.e. the most common words of the language?|`False`|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "looking ---> look\n"
     ]
    }
   ],
   "source": [
    "# Lemmas (the base form of the word):\n",
    "print(f\"{document2[3].text} ---> {document2[3].lemma_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VERB ---> VBG ---> verb, gerund or present participle\n"
     ]
    }
   ],
   "source": [
    "# Simple Parts-of-Speech & Detailed Tags:\n",
    "print(f\"{document2[3].pos_} ---> {document2[3].tag_} ---> {spacy.explain(document2[3].tag_)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harry : Xxxxx\n"
     ]
    }
   ],
   "source": [
    "print(document2[0].text+' : '+document2[0].shape_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Boolean Values:\n",
    "print(document2[0].is_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(document2[0].is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Spans\n",
    "A **span** is a slice of Document object in the form `Document[start:stop]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "document3 = nlp(u\"A paragraph is a group of sentences that fleshes out a single idea. \\\n",
    "In order for a paragraph to be effective, it must begin with a topic sentence, have sentences \\\n",
    "that support the main idea of that paragraph, and maintain a consistent flow.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In order for a paragraph to be effective, it must begin with a topic\n"
     ]
    }
   ],
   "source": [
    "span_ex = document3[14:29]\n",
    "print(span_ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.span.Span"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(span_ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Sentences\n",
    "Certain tokens inside a Document object may also receive a \"start of sentence\" tag. While this doesn't immediately build a list of sentences, these tags enable the generation of sentence segments through `Document.sents`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "document4 = nlp(u'This is the 1st sentence. \\\n",
    "This is 2nd sentence. \\\n",
    "This is the last sentence.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the 1st sentence.\n",
      "This is 2nd sentence.\n",
      "This is the last sentence.\n"
     ]
    }
   ],
   "source": [
    "for sent in document4.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document4[6].is_sent_start"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
